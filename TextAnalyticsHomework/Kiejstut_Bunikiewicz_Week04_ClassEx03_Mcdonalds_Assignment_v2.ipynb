{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Kiejstut_Bunikiewicz_Week04_ClassEx03_Mcdonalds_Assignment_v2.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk1UVKPemrJO"
      },
      "source": [
        "# Class Activity 03 assignment with McDonald's data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8rusLGgmrJX"
      },
      "source": [
        "## Imaginary problem statement\n",
        "\n",
        "McDonald's receives **thousands of customer comments** on their website per day, and many of them are negative. Their corporate employees don't have time to read every single comment, but they do want to read a subset of comments that they are most interested in. In particular, the media has recently portrayed their employees as being rude, and so they want to review comments about **rude service**.\n",
        "\n",
        "McDonald's has hired you to develop a system that ranks each comment by the **likelihood that it is referring to rude service**. They will use your system to build a \"rudeness dashboard\" for their corporate employees, so that employees can spend a few minutes each day examining the **most relevant recent comments**.\n",
        "\n",
        "## Description of the data\n",
        "\n",
        "Before hiring you, McDonald's used the [CrowdFlower platform](http://www.crowdflower.com/data-for-everyone) to pay humans to **hand-annotate** about 1500 comments with the **type of complaint**. The complaint types are listed below, with the encoding used in the data listed in parentheses:\n",
        "\n",
        "- Bad Food (BadFood)\n",
        "- Bad Neighborhood (ScaryMcDs)\n",
        "- Cost (Cost)\n",
        "- Dirty Location (Filthy)\n",
        "- Missing Item (MissingFood)\n",
        "- Problem with Order (OrderProblem)\n",
        "- Rude Service (RudeService)\n",
        "- Slow Service (SlowService)\n",
        "- None of the above (na)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agGc0FSUmrJY"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "Read **`mcdonalds.csv`** into a pandas DataFrame and examine it. (It can be found in the **`data`** directory of the course repository.)\n",
        "\n",
        "- The **policies_violated** column lists the type of complaint. If there is more than one type, the types are separated by newline characters.\n",
        "- The **policies_violated:confidence** column lists CrowdFlower's confidence in the judgments of its human annotators for that row (higher is better).\n",
        "- The **city** column is the McDonald's location.\n",
        "- The **review** column is the actual text comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFzDkp-Mu6fe"
      },
      "source": [
        "#reading McDonald's csv\r\n",
        "import pandas as pd\r\n",
        "path = '/content/mcdonalds.csv'\r\n",
        "mcd = pd.read_csv(path)"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zye1kGM7-pl",
        "outputId": "22ea33b4-6691-48bc-9a74-f1c68d8d49f7"
      },
      "source": [
        "#Examining Data\r\n",
        "mcd.describe()\r\n",
        "mcd.info()\r\n",
        "mcd.head(5)\r\n",
        "mcd['policies_violated'].value_counts().sort_index()"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1525 entries, 0 to 1524\n",
            "Data columns (total 11 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   _unit_id                      1525 non-null   int64  \n",
            " 1   _golden                       1525 non-null   bool   \n",
            " 2   _unit_state                   1525 non-null   object \n",
            " 3   _trusted_judgments            1525 non-null   int64  \n",
            " 4   _last_judgment_at             1525 non-null   object \n",
            " 5   policies_violated             1471 non-null   object \n",
            " 6   policies_violated:confidence  1471 non-null   object \n",
            " 7   city                          1438 non-null   object \n",
            " 8   policies_violated_gold        0 non-null      float64\n",
            " 9   review                        1525 non-null   object \n",
            " 10  Unnamed: 10                   0 non-null      float64\n",
            "dtypes: bool(1), float64(2), int64(2), object(6)\n",
            "memory usage: 120.8+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BadFood                                                                101\n",
              "BadFood\\nCost                                                            4\n",
              "BadFood\\nFilthy                                                          2\n",
              "BadFood\\nFilthy\\nRudeService                                             1\n",
              "BadFood\\nMissingFood                                                     1\n",
              "                                                                      ... \n",
              "na\\nCost                                                                 1\n",
              "na\\nScaryMcDs                                                            2\n",
              "na\\nScaryMcDs\\nBadFood                                                   1\n",
              "na\\nSlowService\\nScaryMcDs                                               1\n",
              "na\\nSlowService\\nScaryMcDs\\nRudeService\\nOrderProblem\\nFilthy\\nCost      1\n",
              "Name: policies_violated, Length: 146, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "y0RzXGNN8iox",
        "outputId": "edfba76f-4df3-4ecd-e13f-60fbcfb76fc6"
      },
      "source": [
        "#Examining text for first row\r\n",
        "mcd.loc[0,'review']"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm not a huge mcds lover, but I've been to better ones. This is by far the worst one I've ever been too! It's filthy inside and if you get drive through they completely screw up your order every time! The staff is terribly unfriendly and nobody seems to care.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QFle-1bmrJY"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "Remove any rows from the DataFrame in which the **policies_violated** column has a **null value**. Check the shape of the DataFrame before and after to confirm that you only removed about 50 rows.\n",
        "\n",
        "- **Note:** Null values are also known as \"missing values\", and are encoded in pandas with the special value \"NaN\". This is distinct from the \"na\" encoding used by CrowdFlower to denote \"None of the above\". Rows that contain \"na\" should **not** be removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlEJ7Gx5u7G2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d39310d-e8d7-4646-a284-cfea28c332b9"
      },
      "source": [
        "#Null Row Removal\r\n",
        "mcd.shape #1525 rows\r\n",
        "mcd.policies_violated.isnull().sum() #54 null rows\r\n",
        "mcd.dropna(subset = ['policies_violated'], inplace = True)\r\n",
        "mcd.shape #1471 rows, so drop successful"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1471, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHeKZJoqmrJZ"
      },
      "source": [
        "## Task 3\n",
        "\n",
        "Add a new column to the DataFrame called **\"rude\"** that is 1 if the **policies_violated** column contains the text \"RudeService\", and 0 if the **policies_violated** column does not contain \"RudeService\". The \"rude\" column is going to be your response variable, so check how many zeros and ones it contains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t8tJX5Ru7xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55c8d82-3bb9-495c-fb9f-34add8aac4f0"
      },
      "source": [
        "# adding \"rude\" column to the dataframe\r\n",
        "rude = []\r\n",
        "for item in mcd['policies_violated']:\r\n",
        "  if 'RudeService' in item:\r\n",
        "    rude.append(1)\r\n",
        "  else:\r\n",
        "    rude.append(0)\r\n",
        "print(rude.count(1))\r\n",
        "print(rude.count(0))\r\n",
        "mcd['rude'] = rude\r\n",
        "mcd['rude'].head() # looks like transferred over, not sure if number of \"Rude\" responses is correct"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "503\n",
            "968\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1\n",
              "1    1\n",
              "2    0\n",
              "3    0\n",
              "4    1\n",
              "Name: rude, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl44k-9rmrJZ"
      },
      "source": [
        "## Task 4\n",
        "\n",
        "1. Define X (the **review** column) and y (the **rude** column).\n",
        "2. Split X and y into training and testing sets (using the parameter **`random_state=1`**).\n",
        "3. Use CountVectorizer (with the **default parameters**) to create document-term matrices from X_train and X_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGID3j_Lu8b1"
      },
      "source": [
        "# 1. define X and y\r\n",
        "X = mcd.review\r\n",
        "y = mcd.rude"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyy2jvaxG-KV"
      },
      "source": [
        "# 2. Splitting into test and training sets\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63jIBSEPHaUQ",
        "outputId": "6ae4d2d4-bfa2-4917-b919-de9d0ea60f7a"
      },
      "source": [
        "# Examining Shape\r\n",
        "print(X_train.shape)\r\n",
        "print(X_test.shape) #split looks right"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1103,)\n",
            "(368,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "benB2GChG-nt"
      },
      "source": [
        "# 3. Using CountVectorizer to create DTM\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "vect = CountVectorizer()"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Ms9gW-H-rO"
      },
      "source": [
        "# fit and transfrom for X_train DTM\r\n",
        "X_train_dtm = vect.fit_transform(X_train)"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt1pz-Q5IDXA"
      },
      "source": [
        "# only transform on X_test DTM\r\n",
        "X_test_dtm = vect.transform(X_test)"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBepr8bRIU7H",
        "outputId": "a442f294-4123-4ce6-857a-7e834d826b93"
      },
      "source": [
        "# Examining Shape and last 50 features\r\n",
        "print(X_train_dtm.shape)\r\n",
        "print(X_test_dtm.shape) #7300 terms extracted\r\n",
        "print(vect.get_feature_names()[-50:])"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1103, 7300)\n",
            "(368, 7300)\n",
            "['œæturns', 'œætwo', 'œæughhh', 'œæughhhhh', 'œæultimately', 'œæum', 'œæunfortunately', 'œæunreal', 'œæuntil', 'œæupon', 'œæuseless', 'œæusually', 'œævery', 'œæwait', 'œæwanna', 'œæwant', 'œæwas', 'œæwasn', 'œæway', 'œæwe', 'œæwell', 'œæwhat', 'œæwhatever', 'œæwhen', 'œæwhich', 'œæwhile', 'œæwho', 'œæwhy', 'œæwill', 'œæwish', 'œæwith', 'œæwon', 'œæword', 'œæwork', 'œæworkers', 'œæworst', 'œæwould', 'œæwow', 'œæwtf', 'œæya', 'œæyay', 'œæyeah', 'œæyears', 'œæyelp', 'œæyep', 'œæyes', 'œæyesterday', 'œæyet', 'œæyou', 'œæyour']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh3juUnbmrJZ"
      },
      "source": [
        "## Task 5\n",
        "\n",
        "Fit a Multinomial Naive Bayes model to the training set, calculate the **predicted probabilites** (not the class predictions) for the testing set, and then calculate the **AUC**.\n",
        "\n",
        "- **Note:** Because McDonald's only cares about ranking the comments by the likelihood that they refer to rude service, **classification accuracy** is not the relevant evaluation metric. **Area Under the Curve (AUC)** is a more useful evaluation metric for this scenario, since it measures the ability of the classifier to assign higher predicted probabilities to positive instances than to negative instances. See [AUC documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html#) for reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlwNp0-vu8_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15426de6-4012-4e21-dce6-6fc5f9528d9d"
      },
      "source": [
        "# Using MultiNomial NB to predict probabilities and calculate AUC\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn import metrics\r\n",
        "\r\n",
        "nb = MultinomialNB()\r\n",
        "nb.fit(X_train_dtm, y_train)\r\n",
        "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1] \r\n",
        "\r\n",
        "print('Features: ', X_train_dtm.shape[1])\r\n",
        "\r\n",
        "#AUC Test\r\n",
        "print(metrics.roc_auc_score(y_test, y_pred_prob))"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  7300\n",
            "0.8426005404546177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x9dmy0tmrJa"
      },
      "source": [
        "## Task 6\n",
        "\n",
        "Using Naive Bayes, try **tuning CountVectorizer** using some of the techniques we learned in class. Check the testing set **AUC** after each change, and find the set of parameters that increases AUC the most.\n",
        "\n",
        "- **Hint:** It is highly recommended that you adapt the **`tokenize_test()`** function from class for this purpose, since it will allow you to iterate quickly through different sets of parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v6P_B9Lu9xm"
      },
      "source": [
        "# Adapting tokenize_test function\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn import metrics\r\n",
        "\r\n",
        "# define a function that accepts a vectorizer and calculates the accuracy\r\n",
        "def tokenize_test(vect):\r\n",
        "    \r\n",
        "    # create document-term matrices using the vectorizer\r\n",
        "    X_train_dtm = vect.fit_transform(X_train)\r\n",
        "    X_test_dtm = vect.transform(X_test)\r\n",
        "    \r\n",
        "    # print the number of features that were generated\r\n",
        "    #look at the shape wihch is the attribute of DTM\r\n",
        "    #shape is the 2nd element, the no. of columns which tells \r\n",
        "    #us howmany features are generated.\r\n",
        "    print('Features: ', X_train_dtm.shape[1])\r\n",
        "    \r\n",
        "    # use Multinomial Naive Bayes to predict the star rating\r\n",
        "    nb = MultinomialNB()\r\n",
        "    nb.fit(X_train_dtm, y_train)\r\n",
        "    y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\r\n",
        "    \r\n",
        "    # print the accuracy of its predictions from the metrics module\r\n",
        "    #pass it to the true values and the predicted values.\r\n",
        "    print(metrics.roc_auc_score(y_test, y_pred_prob))"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd0I21vDQ-Xz",
        "outputId": "c6444a8a-3910-44fe-a8fd-a5c968b62273"
      },
      "source": [
        "#Default Params already used in task 5, doing don't convert to lowercase\r\n",
        "vect = CountVectorizer(lowercase=False)\r\n",
        "tokenize_test(vect) # lowercase has very slightly decreased the AUC"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  8742\n",
            "0.8406453663964394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd_9vcLERI4x",
        "outputId": "c5ed5533-4e1c-4fc7-c693-de90b059f1d7"
      },
      "source": [
        "#including 1 and 2 grams\r\n",
        "vect = CountVectorizer(ngram_range=(1, 2))\r\n",
        "tokenize_test(vect) #This has significantly decreased the AUC"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  57936\n",
            "0.8195994277539342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BgvqQ3-RNI4",
        "outputId": "dfc864ca-375b-47b7-cc35-eeacfe917a96"
      },
      "source": [
        "# removing stop words\r\n",
        "vect = CountVectorizer(stop_words='english')\r\n",
        "tokenize_test(vect) # Removing the common english stopwords has slightly increased the AUC"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  7020\n",
            "0.853520902877126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpG5OMnXRVyH",
        "outputId": "6c5c382e-662e-4f6f-efe0-76bc75496414"
      },
      "source": [
        "#removing doc-specific stop words (those appearing in more than 30% of documents)\r\n",
        "vect = CountVectorizer(max_df=0.3)\r\n",
        "tokenize_test(vect) #this has marginally increased AUC"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  7269\n",
            "0.8523764107455094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGMDvqVfTCOg",
        "outputId": "17217076-6b65-455b-e022-2bfafa392069"
      },
      "source": [
        "#Only keeping items in at least 3 documents\r\n",
        "vect = CountVectorizer(min_df=3)\r\n",
        "tokenize_test(vect) #this has not increased AUC much"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  2449\n",
            "0.8430456207280242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uHqdzbtTKO7",
        "outputId": "9c69c96e-5ecd-4f78-9cbb-faa825135472"
      },
      "source": [
        "#Combination of factors\r\n",
        "vect = CountVectorizer(stop_words = 'english', max_df=0.3, min_df = 6, lowercase= False, ngram_range=(1,2))\r\n",
        "tokenize_test(vect)# this looks to be the best performing model I could find"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  1651\n",
            "0.8558416785884597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-IqSOBJa8f8",
        "outputId": "878fe987-d802-437b-8dd1-71d9ba111198"
      },
      "source": [
        "#Combination of best performers\r\n",
        "vect = CountVectorizer(stop_words = 'english', max_df=0.3, min_df = 2, lowercase= False)\r\n",
        "tokenize_test(vect) "
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  3745\n",
            "0.8583849944364966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMbPrFM3mrJa"
      },
      "source": [
        "## Task 7 (Optional)\n",
        "\n",
        "The **city** column might be predictive of the response, but we are not currently using it as a feature. Let's see whether you can increase the AUC by adding it to the model:\n",
        "\n",
        "1. Create a new DataFrame column, **review_city**, that concatenates the **review** text with the **city** text. One easy way to combine string columns in pandas is by using the [`Series.str.cat()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.cat.html) method. Make sure to use the **space character** as a separator, as well as replacing **null city values** with a reasonable string value (such as 'na').\n",
        "2. Redefine X as the **review_city** column, and re-split X and y into training and testing sets.\n",
        "3. When you run **`tokenize_test()`**, CountVectorizer will simply treat the city as an extra word in the review, and thus it will automatically be included in the model! Check to see whether it increased or decreased the AUC of your **model**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "4gP4l03sX5qi",
        "outputId": "fca82915-a087-4461-e47e-cdb427e9d202"
      },
      "source": [
        "# 1. Creating review_city column\r\n",
        "mcd['review_city'] = mcd['review'].str.cat(mcd['city'], join = 'outer', sep = ' ', na_rep='na')\r\n",
        "mcd['review_city'][0] # looks like the text carried over correctly"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm not a huge mcds lover, but I've been to better ones. This is by far the worst one I've ever been too! It's filthy inside and if you get drive through they completely screw up your order every time! The staff is terribly unfriendly and nobody seems to care. Atlanta\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6t75rFhgXko",
        "outputId": "1099172c-6e24-4206-b21a-50e68d1c7e19"
      },
      "source": [
        "# 2. Redefining X with the review_city  and creating new training and testing sets\r\n",
        "X = mcd.review_city\r\n",
        "y = mcd.rude\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\r\n",
        "\r\n",
        "print(X_train.shape)\r\n",
        "print(X_test.shape) #split looks right\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "vect = CountVectorizer()"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1103,)\n",
            "(368,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsToC8plg05I",
        "outputId": "2a7c9724-ddf3-4f8a-9f85-86b1cba8e758"
      },
      "source": [
        "# using the tokenize test to see if accuracy has increased compared to default of 0.842\r\n",
        "vect = CountVectorizer()\r\n",
        "tokenize_test(vect) #this has increased the AUC by 0.0002, so not much, but only 3 new terms were added"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  7303\n",
            "0.8428071848672706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQAVkQYzmrJc"
      },
      "source": [
        "## Task 8 (Optional)\n",
        "\n",
        "New comments have been submitted to the McDonald's website, and you need to **score them with the likelihood** that they are referring to rude service.\n",
        "\n",
        "1. Before making predictions on out-of-sample data, it is important to re-train your model on all relevant data using the tuning parameters and preprocessing steps that produced the best AUC above.\n",
        "    - In other words, X should be defined using either **all rows** or **only those rows with a confidence_mean of at least 0.75**, whichever produced a better AUC above.\n",
        "    - X should refer to either the **review column** or the **review_city column**, whichever produced a better AUC above.\n",
        "    - CountVectorizer should be instantiated with the **tuning parameters** that produced the best AUC above.\n",
        "    - **`train_test_split()`** should not be used during this process.\n",
        "2. Build a document-term matrix (from X) called **X_dtm**, and examine its shape.\n",
        "3. Read the new comments stored in **`mcdonalds_new.csv`** into a DataFrame called **new_comments**, and examine it.\n",
        "4. If your model uses a **review_city** column, create that column in the new_comments DataFrame. (Otherwise, skip this step.)\n",
        "5. Build a document_term matrix (from the **new_comments** DataFrame) called **new_dtm**, and examine its shape.\n",
        "6. Train your best model (Naive Bayes) using **X_dtm** and **y**.\n",
        "7. Predict the \"rude probability\" for each comment in **new_dtm**, and store the probabilities in an object called **new_pred_prob**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol-6Srr8iogn"
      },
      "source": [
        "# 1. Re-train model ###Could not figure out this task###\r\n",
        "X = mcd.review_city\r\n",
        "y = mcd.rude\r\n",
        "\r\n",
        "#Creating new training sets\r\n",
        "X_train = X.sample(frac=0.75)\r\n",
        "y_train = y.sample(frac= 0.75)\r\n",
        "X_test = X.sample(frac=0.25)\r\n",
        "y_test = y.sample(frac = 0.21)\r\n",
        "\r\n",
        "vect = CountVectorizer(stop_words = 'english', max_df=0.3, min_df = 2, lowercase= False)"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZRDr73Vi5Ad"
      },
      "source": [
        "# 2. Build a new dtm\r\n",
        "X_dtm = vect.fit_transform(X_train)"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZBhhG5Vi5P1",
        "outputId": "ad55ecb4-949e-459f-a7e1-f8b749fd041b"
      },
      "source": [
        "# 3. Read in new comments\r\n",
        "new_comments = pd.read_csv('/content/mcdonalds_new.csv')\r\n",
        "\r\n",
        "new_comments.head()\r\n",
        "new_comments['review'][0]\r\n",
        "new_comments.shape"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "OBOXk6nfi5tQ",
        "outputId": "88054591-bc85-4034-8466-d371276c8478"
      },
      "source": [
        "# 4. Create review_city column\r\n",
        "new_comments['review_city'] = new_comments['review'].str.cat(new_comments['city'], join = 'outer', sep = ' ', na_rep='na')\r\n",
        "new_comments['review_city'][0] # looks like the text carried over correctly"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Went through the drive through and ordered a #10 (cripsy sweet chili chicken wrap) without fries- the lady couldn't understand that I did not want fries and charged me for them anyways. I got the wrong order- a chicken sandwich and a large fries- my boyfriend took it back inside to get the correct order. The gentleman that ordered the chicken sandwich was standing there as well and she took the bag from my bf- glanced at the insides and handed it to the man without even offering to replace. I mean with all the scares about viruses going around... ugh DISGUSTING SERVICE. Then when she gave him the correct order my wrap not only had the sweet chili sauce on it, but the nasty (just not my first choice) ranch dressing on it!!!! I mean seriously... how lazy can you get!!!! I worked at McDonalds in Texas when I was 17 for about 8 months and I guess I was spoiled with good management. This was absolutely ridiculous. I was beyond disappointed. Las Vegas\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IrFKH31i6Ja",
        "outputId": "a182ac4f-281b-45cc-abf7-c40f9a9fc64d"
      },
      "source": [
        "# 5. Creating new_dtm\r\n",
        "new_dtm = vect.transform(new_comments)\r\n",
        "new_dtm.shape"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3769)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "VVUnyUqBi6kE",
        "outputId": "ec26f4d6-aa8c-48fc-a783-79865da22d21"
      },
      "source": [
        "# 6. Training Naive Bayes with new info \r\n",
        "# Using MultiNomial NB to predict probabilities and calculate AUC\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn import metrics\r\n",
        "\r\n",
        "nb = MultinomialNB()\r\n",
        "nb.fit(X_train, new_comments)"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-260-8c4f67ca25b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \"\"\"\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    795\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    796\u001b[0m         \"\"\"\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Went for the restroom, and was disappointed by the quality and service.They charged me for Mac sauce on my burger. There was no sauce on my burger. New York'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__Ztyhqvi686"
      },
      "source": [
        "# 7. Predict Prob\r\n",
        "\r\n",
        "#take prediction column and append each item to the list new_pred_prob\r\n",
        "\r\n",
        "y_pred_prob = nb.predict_proba(new_dtm)\r\n",
        "\r\n",
        "new_pred_prob = []\r\n",
        "for item in y_pred_prob[:, 1]:\r\n",
        "  new_pred_prob.append(item)\r\n",
        "\r\n",
        "new_pred_prob\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}